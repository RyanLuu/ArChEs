{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Enumerative Search in Python\n",
    "\n",
    "This is a way for us to quickly test if a given task *can* have a program synthesized for it through naive, recursive enumeration up to a given depth. \n",
    "\n",
    "Author: Sean Flannery\n",
    "Email: sflanner@purdue.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Currently supported DSL\n",
    "```\n",
    "program ::= single\n",
    "single ::=  filterColor(single, color) \n",
    "\t\t\t\t\t\t| recolor(single, color)\n",
    "\t\t\t\t\t\t| orthogonal(single, axis)            \n",
    "\t\t\t\t\t\t| image\n",
    "color ::= 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9\n",
    "axis  ::= Y_AXIS | X_AXIS | ROT_90 \n",
    "\n",
    "Removed XY_AXIS, YX_AXIS, ROT_270, ROT_180 since they can be expressed in our DSL already\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from copy import copy\n",
    "from functools import lru_cache\n",
    "from tqdm.notebook import tqdm\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility for Fetching our Task Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given path to a file containing our data, return 2 lists\n",
    "# 1 containing all training examples as tuple pairs of numpy arrays\n",
    "# 1 containing all test examples as tuple pairs of numpy arrays \n",
    "def get_task_data(json_path):\n",
    "    with open(json_path) as json_file_reader:\n",
    "        json_dict = json.loads(json_file_reader.read())\n",
    "        # get our input/output examples\n",
    "        train_examples = []\n",
    "        for d in json_dict['train']:\n",
    "            train_examples.append((np.array(d['input']),np.array(d['output'])))\n",
    "        test_examples = []\n",
    "        for d in json_dict['test']:\n",
    "            test_examples.append((np.array(d['input']),np.array(d['output'])))   \n",
    "        return train_examples, test_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample: Reflection on X, then Y axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = get_task_data('data/training/9dfd6313.json') # 9dfd6313.json is a reflection on XY axis task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity(image, extra_arg=None):\n",
    "    # anywhere the color is, keep it, else set to 0\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterColor(image, color):\n",
    "    # anywhere the color is, keep it, else set to 0\n",
    "    return np.where(image == color, image, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recolor(image, color):\n",
    "    # set all nonzero pixels to color, else keep it the same (0)\n",
    "    return np.where(image != 0, color, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orthogonal(image, axis):\n",
    "    if   axis == 'Y_AXIS':\n",
    "        return np.fliplr(image)\n",
    "    elif axis == 'X_AXIS':\n",
    "        return np.flipud(image)\n",
    "    elif axis == 'ROT_90':\n",
    "        return np.rot90(image,1) # rotate once\n",
    "    #elif axis == 'ROT_180': \n",
    "    #    return np.rot90(image,2) # rotate twice\n",
    "    #elif axis == 'ROT_270':\n",
    "    #    return np.rot90(image,3) # rotate thrice\n",
    "    else:\n",
    "        raise NotImplemented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]\n",
      " [0 2]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1,2],[3,4],[0,2]])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1]\n",
      " [4 3]\n",
      " [2 0]]\n"
     ]
    }
   ],
   "source": [
    "print(orthogonal(a, 'Y_AXIS'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2]\n",
      " [3 4]\n",
      " [1 2]]\n"
     ]
    }
   ],
   "source": [
    "print(orthogonal(a, 'X_AXIS'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 4 2]\n",
      " [1 3 0]]\n"
     ]
    }
   ],
   "source": [
    "print(orthogonal(a, 'ROT_90'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2],\n",
       "       [0, 0],\n",
       "       [0, 2]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filterColor(a,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 4],\n",
       "       [4, 4],\n",
       "       [0, 4]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recolor(a,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compose all options that our program could take at any level\n",
    "\n",
    "incomplete: We still have to make this recursive.\n",
    "\n",
    "idea: Structure each enumeration as a tuple like\n",
    "\n",
    "if we want to use the raw input:\n",
    "```\n",
    "(function, None, args)\n",
    "```\n",
    "if we want to use input from a greater depth:\n",
    "```\n",
    "(function, program_output_from_greater_depth, args)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_options = [filterColor, recolor, orthogonal]\n",
    "color_options = list(range(10)) #todo: remove last option?\n",
    "orth_options = ['Y_AXIS', 'X_AXIS', 'ROT_90']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_enumerations = list(product([filterColor, recolor], list(range(10))))\n",
    "orth_enumerations = list(product([orthogonal], orth_options))\n",
    "#identity_options = [(identity, None)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_search_options = color_enumerations + orth_enumerations #+ identity_options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important globals to always set with each run of the below functions\n",
    "```\n",
    "SEARCH_OPTIONS - the possible functions that could be applied at any level \n",
    "NUM_SEARCH_OPTIONS - len of the prior argument\n",
    "TASK_PAIRS - a copy of the task pairs for the desired task\n",
    "MAX_LRU_SIZE - max number of LRU entries to keep in memory for run_program_on_task\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_OPTIONS = total_search_options\n",
    "NUM_SEARCH_OPTIONS = len(total_search_options)\n",
    "TASK_PAIRS = copy(train) # make a copy of our training data and specify what task we want to work on \n",
    "MAX_LRU_SIZE=1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to generate a program from a number\n",
    "\n",
    "This creates a python generator of arbitrary depth WITHOUT loading all digits into memory. This is especially important when our enumeration size is massive..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def programGenerator(num, depth):\n",
    "    # given a number, and the depth level, we can determine which of the options they wanted\n",
    "    inner_arg = None # None indicates we should pass the input itself\n",
    "    for _ in range(depth):\n",
    "        option = SEARCH_OPTIONS[num % NUM_SEARCH_OPTIONS]\n",
    "        # get the function, and any additional arguments\n",
    "        func = option[0]\n",
    "        args = option[1] # TODO: Expand this beyond just the 1\n",
    "        inner_arg = (func, inner_arg, args)\n",
    "        num = num // NUM_SEARCH_OPTIONS\n",
    "    return inner_arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_maybe = (programGenerator(num,depth=6) for num in range(NUM_SEARCH_OPTIONS**6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<function __main__.filterColor(image, color)>,\n",
       " (<function __main__.filterColor(image, color)>,\n",
       "  (<function __main__.filterColor(image, color)>,\n",
       "   (<function __main__.filterColor(image, color)>,\n",
       "    (<function __main__.filterColor(image, color)>,\n",
       "     (<function __main__.filterColor(image, color)>, None, 0),\n",
       "     0),\n",
       "    0),\n",
       "   0),\n",
       "  0),\n",
       " 0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(gen_maybe) # This is what would happen if we picked 0 at each level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Run a Given Program on our Task\n",
    "\n",
    "We use an LRU_CACHE to automatically memoize and prevent us from repeating work on programs we've seen before...\n",
    "\n",
    "For example, setting a MAXSIZE of 1024, means that we'll hold at most 1024 input hashes and their corresponding outputs in memory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 1: No LRU Cache (np.arrays can't be cached)\n",
    "This is the version that is more intuitive for someone to check things after a program is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_program_on_task(task_input, true_output, program):\n",
    "    # Recurse here for sub-programs (don't worry, we memoized)\n",
    "    if program[1] is not None: # check if we have an inner arg: \n",
    "        inner_was_valid, task_input = run_program_on_task(task_input, true_output, program[1])\n",
    "        if inner_was_valid: # a subprogram was correct!\n",
    "            return True, program[1]\n",
    "    # check that program_output and true_output are the same\n",
    "    # - program[0] is a reference to a function\n",
    "    # - program[2] are the additional arguments for the function\n",
    "    program_output = program[0](task_input,program[2]) \n",
    "    return np.array_equal(program_output, true_output), program_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 2: Heck yeah, some LRU Cache (use the index to the given task's np.arrays, since those are hashable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=MAX_LRU_SIZE) # auto-memoization, but requires globals for inputs/outputs\n",
    "def run_program_on_task_lru_safe(io_index, program):\n",
    "    global TASK_PAIRS\n",
    "    task_input, true_output = TASK_PAIRS[io_index]\n",
    "    # Recurse here for sub-programs (don't worry, we memoized)\n",
    "    if program[1] is not None: # check if we have an inner arg: \n",
    "        inner_was_valid, task_input = run_program_on_task_lru_safe(io_index, program[1])\n",
    "        if inner_was_valid: # a subprogram was correct!\n",
    "            return True, program[1]\n",
    "    # check that program_output and true_output are the same\n",
    "    # - program[0] is a reference to a funciton\n",
    "    # - program[2] are the additional arguments for the function\n",
    "    program_output = program[0](task_input,program[2]) \n",
    "    return np.array_equal(program_output, true_output), program_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enumerative Search at given depth\n",
    "Make sure the globals for TASK_PAIRS and SEARCH_OPTIONS are set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enumerative_search(depth=1): \n",
    "    global TASK_PAIRS\n",
    "    global SEARCH_OPTIONS\n",
    "    global NUM_SEARCH_OPTIONS\n",
    "    task_range = range(len(TASK_PAIRS))\n",
    "    # Create a generator that we can use to evaluate all of our potential programs\n",
    "    program_generator = (programGenerator(num,depth=depth) for num in range(NUM_SEARCH_OPTIONS**depth))\n",
    "    for attempt_index, program in enumerate(program_generator):\n",
    "        # TODO: turn this into a function that is parallelizable\n",
    "        # let's see if this attempt works!\n",
    "        all_valid = True\n",
    "        for io_index in task_range:\n",
    "            is_valid, _ = run_program_on_task_lru_safe(io_index, program)\n",
    "            if not is_valid:\n",
    "                all_valid = False\n",
    "                break\n",
    "        # if all of it worked out, return the number, and the program itself \n",
    "        if all_valid:\n",
    "            return attempt_index, program\n",
    "    # nothing was generated. return number of programs checked\n",
    "    return NUM_SEARCH_OPTIONS**depth, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lru_cache memoizes recursive calls (up to MAXSIZE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A function to print out a prettier version of our generated functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prettify(program_tuple):\n",
    "    if program_tuple is None:\n",
    "        return \"Failed to generate a program.\"\n",
    "    func_name = program_tuple[0].__name__\n",
    "    args = program_tuple[2]\n",
    "    # check if it was recursive\n",
    "    if program_tuple[1] is not None:\n",
    "        return f\"{func_name}({prettify(program_tuple[1])},{args})\"\n",
    "    else:\n",
    "        return f\"{func_name}(input,{args})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'orthogonal(orthogonal(input,ROT_90),X_AXIS)'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attempt_index, program = enumerative_search(2)\n",
    "prettify(program)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An all-in-one function to quickly test an ARC task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_task_solution(task_path, max_depth=4, search_options=total_search_options, max_lru_size=None): \n",
    "    global TASK_PAIRS\n",
    "    global SEARCH_OPTIONS\n",
    "    global NUM_SEARCH_OPTIONS\n",
    "    global MAX_LRU_SIZE\n",
    "    \n",
    "    if max_lru_size is None:\n",
    "        MAX_LRU_SIZE = min(NUM_SEARCH_OPTIONS**3,NUM_SEARCH_OPTIONS**(max_depth-1))\n",
    "    \n",
    "    SEARCH_OPTIONS = search_options\n",
    "    NUM_SEARCH_OPTIONS = len(SEARCH_OPTIONS)\n",
    "    MAX_LRU_SIZE = max_lru_size\n",
    "    train, test = get_task_data(task_path)\n",
    "    TASK_PAIRS = copy(train) # we only consider training examples\n",
    "    \n",
    "    # We want to clear the cache of any prior examples in case that was for a different task\n",
    "    run_program_on_task_lru_safe.cache_clear()\n",
    "    counter = 0\n",
    "    for depth in range(1,max_depth + 1):\n",
    "        num_checked, program = enumerative_search(depth)\n",
    "        counter += num_checked\n",
    "        if program is not None:\n",
    "            break\n",
    "            \n",
    "    # check if it was None\n",
    "    if program is None:\n",
    "        return None, \"TRAIN_FAILED\", counter\n",
    "    \n",
    "    # Now, since we have a program that worked on training data, let's see what happens on test data\n",
    "    for test_input, true_test_output in test:\n",
    "        is_valid, _ = run_program_on_task(test_input, true_test_output, program)\n",
    "        if not is_valid:\n",
    "            return program, \"TEST_FAILED\", counter\n",
    "        \n",
    "    # Ah, yes, very nice\n",
    "    return program, \"SUCCESS\", counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS\n",
      "Number of programs checked: 528\n",
      "Generated Program:\n",
      "\n",
      "orthogonal(orthogonal(input,ROT_90),X_AXIS)\n"
     ]
    }
   ],
   "source": [
    "program, msg, ctr = generate_task_solution('data/training/9dfd6313.json')\n",
    "print(msg)\n",
    "print(f\"Number of programs checked: {ctr}\")\n",
    "print(f\"Generated Program:\\n\\n{prettify(program)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying all of our training examples\n",
    "feel free to change the max_depth!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82a0938139814af2bf89f984a6a429cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/179 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATISTICS FOR GIVEN DSL WITH MAX DEPTH 3\n",
      "SUCCESS: 7\n",
      "TEST_FAILED: 0\n",
      "TRAIN_FAILED: 172\n",
      "\n",
      "data/training/ed36ccf7.json: orthogonal(input,ROT_90)\n",
      "data/training/74dd1130.json: orthogonal(orthogonal(input,ROT_90),X_AXIS)\n",
      "data/training/6150a2bd.json: orthogonal(orthogonal(input,X_AXIS),Y_AXIS)\n",
      "data/training/68b16354.json: orthogonal(input,X_AXIS)\n",
      "data/training/3c9b0459.json: orthogonal(orthogonal(input,X_AXIS),Y_AXIS)\n",
      "data/training/67a3c6ac.json: orthogonal(input,Y_AXIS)\n",
      "data/training/9dfd6313.json: orthogonal(orthogonal(input,ROT_90),X_AXIS)\n"
     ]
    }
   ],
   "source": [
    "SOURCE_DIR_PATH = 'data/training/'\n",
    "files_to_try = [f\"{SOURCE_DIR_PATH}{file}\" for file in os.listdir('data/training/') if os.path.isfile(f\"{SOURCE_DIR_PATH}{file}\")]\n",
    "results = {'SUCCESS':[], 'TEST_FAILED':[], 'TRAIN_FAILED':[]}\n",
    "\n",
    "for task_index, task_path in tqdm(enumerate(files_to_try),total=len(files_to_try)):\n",
    "    program, msg, ctr = generate_task_solution(task_path, max_depth=3)\n",
    "    results[msg].append((task_path, program))\n",
    "\n",
    "print(\n",
    "f'''STATISTICS FOR GIVEN DSL WITH MAX DEPTH 3\n",
    "SUCCESS: {len(results['SUCCESS'])}\n",
    "TEST_FAILED: {len(results['TEST_FAILED'])}\n",
    "TRAIN_FAILED: {len(results['TRAIN_FAILED'])}\n",
    "'''\n",
    ")\n",
    "\n",
    "print('\\n'.join([f\"{task_path}: {prettify(program)}\" for task_path, program in results['SUCCESS']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1943805903e446b7979186ee4c484066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/179 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATISTICS FOR GIVEN DSL WITH MAX DEPTH 4\n",
      "SUCCESS: 7\n",
      "TEST_FAILED: 1\n",
      "TRAIN_FAILED: 171\n",
      "\n",
      "data/training/ed36ccf7.json: orthogonal(input,ROT_90)\n",
      "data/training/74dd1130.json: orthogonal(orthogonal(input,ROT_90),X_AXIS)\n",
      "data/training/6150a2bd.json: orthogonal(orthogonal(input,X_AXIS),Y_AXIS)\n",
      "data/training/68b16354.json: orthogonal(input,X_AXIS)\n",
      "data/training/3c9b0459.json: orthogonal(orthogonal(input,X_AXIS),Y_AXIS)\n",
      "data/training/67a3c6ac.json: orthogonal(input,Y_AXIS)\n",
      "data/training/9dfd6313.json: orthogonal(orthogonal(input,ROT_90),X_AXIS)\n"
     ]
    }
   ],
   "source": [
    "SOURCE_DIR_PATH = 'data/training/'\n",
    "files_to_try = [f\"{SOURCE_DIR_PATH}{file}\" for file in os.listdir('data/training/') if os.path.isfile(f\"{SOURCE_DIR_PATH}{file}\")]\n",
    "results = {'SUCCESS':[], 'TEST_FAILED':[], 'TRAIN_FAILED':[]}\n",
    "\n",
    "for task_index, task_path in tqdm(enumerate(files_to_try),total=len(files_to_try)):\n",
    "    program, msg, ctr = generate_task_solution(task_path, max_depth=4)\n",
    "    results[msg].append((task_path, program))\n",
    "\n",
    "print(\n",
    "f'''STATISTICS FOR GIVEN DSL WITH MAX DEPTH 4\n",
    "SUCCESS: {len(results['SUCCESS'])}\n",
    "TEST_FAILED: {len(results['TEST_FAILED'])}\n",
    "TRAIN_FAILED: {len(results['TRAIN_FAILED'])}\n",
    "'''\n",
    ")\n",
    "\n",
    "print('\\n'.join([f\"{task_path}: {prettify(program)}\" for task_path, program in results['SUCCESS']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/training/f76d97a5.json: recolor(recolor(recolor(filterColor(input,5),9),6),4)\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join([f\"{task_path}: {prettify(program)}\" for task_path, program in results['TEST_FAILED']]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
